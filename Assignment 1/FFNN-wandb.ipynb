{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl1 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "source": [
        "## About Notebook:\n",
        "\n",
        "This notebook is for running sweeps on wandb wsing google colab."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FrRC5VFyvyl"
      },
      "source": [
        "import numpy as np\n",
        "# import pandas as pd\n",
        "import time\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import fashion_mnist\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "# ACTIVATION FUNCTIONS\n",
        "class sigmoid():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def activate(self, x):\n",
        "        return 1/(1 + np.exp(-x))\n",
        "\n",
        "    def grad(self, x):\n",
        "        return self.activate(x)*(1 - self.activate(x))\n",
        "\n",
        "class relu():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def activate(self, x):\n",
        "        return np.maximum(x,0)\n",
        "  \n",
        "    def grad(self, x):\n",
        "        x[x>0] = 1\n",
        "        x[x<0] = 0\n",
        "        return x\n",
        "\n",
        "class tanh():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def activate(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def grad(self, x):\n",
        "        return (1 -(np.tanh(x)**2))\n",
        "\n",
        "class softmax():\n",
        "    def __init__(self):\n",
        "        self.name='softmax'\n",
        "        pass\n",
        "    def activate(self, x):\n",
        "        out = np.zeros(x.shape)\n",
        "        for i in range(0, x.shape[0]):\n",
        "            exps = np.exp(x[i]- np.max(x[i]))\n",
        "            out[i] = exps / np.sum(exps)        \n",
        "        return out\n",
        "\n",
        "    def grad(self, x):  \n",
        "        pass    \n",
        "\n",
        "\n",
        "## LOSS FUNCTIONS\n",
        "class crossEntropy():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def loss(self,y_pred,y_true):\n",
        "        n = len(y_pred)\n",
        "        y_true_1 = [np.where(temp == 1) for temp in y_true]\n",
        "        loss = np.array([-np.log(y_pred[i][y_true_1[i]]) for i in range(n)])\n",
        "        return np.mean(loss)\n",
        "    def loss_grad(self,y_pred,y_true):\n",
        "        loss_grad = y_pred - y_true\n",
        "        return loss_grad\n",
        "     \n",
        "class meanSq():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def solve(self,a,b):\n",
        "        c = a - b\n",
        "        d = c*a\n",
        "        s = np.sum(d)\n",
        "        return d - s*a    \n",
        "    def loss(self,y_pred,y_true):\n",
        "        loss = 0.5*np.sum(np.square(y_pred-y_true),axis=1)\n",
        "        return np.mean(loss)    \n",
        "    def loss_grad(self,y_pred,y_true):\n",
        "        n = len(y_pred)\n",
        "        loss_grad = np.array([self.solve(y_pred[i],y_true[i]) for i in range(n)])\n",
        "        return loss_grad\n",
        "\n",
        "class layer():\n",
        "    def __init__(self,inp,out,act,wbinit, optimizer, eta, wd):\n",
        "      self.prev_n=inp\n",
        "      self.curr_n=out\n",
        "      self.activation=act\n",
        "      self.wb_initializer=wbinit\n",
        "      self.wd = wd\n",
        "      self.grad_w, self.grad_b = 0,0\n",
        "      if optimizer=='sgd':\n",
        "        self.optimizer = SGDOptim(eta = eta, wd = self.wd)\n",
        "      elif optimizer=='mgd':\n",
        "        self.optimizer = MomentGDOptim(eta = eta, wd = self.wd)\n",
        "      elif optimizer=='nag':\n",
        "        self.optimizer = NAGDOptim(eta = eta, wd = self.wd)\n",
        "      elif optimizer=='rms':\n",
        "        self.optimizer = RMSOptim(eta = eta, wd = self.wd)\n",
        "      elif optimizer=='adam':\n",
        "        self.optimizer = adamOptim(eta = eta, wd = self.wd)\n",
        "      elif optimizer=='nadam':\n",
        "        self.optimizer = nadamOptim(eta = eta, wd = self.wd)\n",
        "      self.initialize_wb()\n",
        "    \n",
        "    def initialize_wb(self):\n",
        "      self.w = self.wb_initializer.init_w(self.prev_n,self.curr_n)\n",
        "      self.b = np.zeros(self.curr_n)\n",
        "\n",
        "    def get_grad_w(self,a,b):\n",
        "      c = np.dot(b.T, a)\n",
        "      return c\n",
        "\n",
        "    def get_grad_b(self,a):\n",
        "      c=np.mean(a,axis=0)\n",
        "      return c\n",
        "\n",
        "    def get_grad_h(self,a,b):\n",
        "      c=np.einsum(\"ij,kj->ki\",a,b)\n",
        "      return c\n",
        "\n",
        "    def get_grad_a(self,a,b):\n",
        "      c=a*b\n",
        "      return c\n",
        "    \n",
        "    def get_derivs(self,a,act):\n",
        "      return act.grad(a)\n",
        "    \n",
        "    def frwd(self,inputs):\n",
        "      self.input=inputs\n",
        "      self.a = np.dot(self.input,self.w)+self.b\n",
        "      self.h = self.activation.activate(self.a)\n",
        "      return self.h\n",
        "\n",
        "    def bkwd(self,grad_a,prev_layer_a,prev_act,i):\n",
        "      self.grad_w += self.get_grad_w(grad_a,self.input)\n",
        "      #self.grad_b=np.mean(grad_a,axis=0)\n",
        "      self.grad_b += np.mean(grad_a, axis=0) * self.input.shape[0] \n",
        "      if i==0:\n",
        "        return grad_a\n",
        "      grad_a=np.dot(grad_a,self.w.T)*prev_act.grad(prev_layer_a)\n",
        "      return grad_a\n",
        "    \n",
        "    def update_wb(self,t):\n",
        "      self.w, self.b = self.optimizer.update(w = self.w, b=self.b, dw=self.grad_w, db=self.grad_b, t=t)\n",
        "      self.grad_w , self.grad_b = 0,0\n",
        "\n",
        "    def partial_wb(self):\n",
        "      v_w, v_b = self.optimizer.partial()\n",
        "      self.w = self.w - v_w\n",
        "      self.b = self.b - v_b\n",
        "    def get_act(self):\n",
        "      return self.activation\n",
        "    def get_a(self):\n",
        "      return self.a\n",
        "\n",
        "\n",
        "class nn():\n",
        "    def __init__(self,input_size,output_size,neuronlist,batch_size,epochs,optimizer,loss_function,learning_rate,wb_initializer, weight_decay):\n",
        "      self.network=[]\n",
        "      self.batch_size=batch_size\n",
        "      self.learning_rate=learning_rate\n",
        "      self.epochs=epochs\n",
        "      self.wd = weight_decay\n",
        "\n",
        "      fl=[[input_size,'dummystr']]+neuronlist+[[output_size,'softmax']]\n",
        "\n",
        "      self.optimizer = optimizer\n",
        "\n",
        "      if loss_function=='crossentropy':\n",
        "        self.loss_function=crossEntropy()\n",
        "      elif loss_function=='meansq':\n",
        "        self.loss_function=meanSq()\n",
        "      if wb_initializer=='xavier':\n",
        "        self.wb_initializer=xavier()\n",
        "      elif wb_initializer=='random':\n",
        "        self.wb_initializer=randwb()\n",
        "\n",
        "      for i in range(len(fl)-1):\n",
        "        actstr=fl[i+1][1]\n",
        "        if actstr=='sigmoid':\n",
        "          activation=sigmoid()\n",
        "        elif actstr=='tanh':\n",
        "          activation=tanh()\n",
        "        elif actstr=='relu':\n",
        "          activation=relu()\n",
        "        elif actstr=='softmax':\n",
        "          activation=softmax()\n",
        "        self.network.append(layer(inp=fl[i][0],out=fl[i+1][0],act=activation,wbinit=self.wb_initializer, optimizer = self.optimizer, eta=self.learning_rate, wd=self.wd))\n",
        "\n",
        "    def forward(self,X):\n",
        "      for layer in self.network:\n",
        "        X=layer.frwd(X)\n",
        "      return X\n",
        "      \n",
        "    def process_y(self,y):\n",
        "      y_t = np.zeros((y.shape[0], 10))\n",
        "      for i in range(y.shape[0]):\n",
        "        y_t[i][y[i]]=1\n",
        "      return y_t\n",
        "\n",
        "    def fit_batch(self,X,y,t):\n",
        "      y_pred=self.forward(X)\n",
        "      y_true=self.process_y(y)\n",
        "      grad_a=self.loss_function.loss_grad(y_pred,y_true)\n",
        "      for i in range(len(self.network)-1,-1,-1):\n",
        "        layer=self.network[i]\n",
        "        if layer.optimizer.name=='nag':\n",
        "          layer.partial_wb()\n",
        "        else:\n",
        "          pass\n",
        "        if i!=0:\n",
        "          grad_a=layer.bkwd(grad_a,self.network[i-1].get_a(),self.network[i-1].get_act(),i)\n",
        "        else:\n",
        "          grad_a=layer.bkwd(grad_a,self.network[0].get_a(),self.network[0].get_act(),i)\n",
        "        layer.update_wb(t)\n",
        "\n",
        "    def fit(self,X,y, Xval, yval):\n",
        "      for ep in range(self.epochs):\n",
        "        X, y = shuffle(X, y, random_state=ep)\n",
        "        for i in range(0,X.shape[0],self.batch_size):\n",
        "          x_batch = X[i:i + self.batch_size]\n",
        "          y_batch = y[i:i + self.batch_size]\n",
        "          self.fit_batch(x_batch, y_batch,(i/self.batch_size))\n",
        "        y_true = self.process_y(y)\n",
        "        acc_t=(np.mean(self.predict(X).argmax(axis=-1)==y_true.argmax(axis=-1)))\n",
        "        y_pval = self.predict(Xval)\n",
        "        y_tval = self.process_y(yval)\n",
        "        acc_v=(np.mean(y_pval.argmax(axis=-1)==y_tval.argmax(axis=-1)))\n",
        "        #print(y_true)\n",
        "        print(\"Epoch: \"+str(ep+1)+\", Train accuracy: \"+str(acc_t)+\" Val accuracy : \"+str(acc_v))\n",
        "    \n",
        "    def fit_epoch(self, X, y, epoch):\n",
        "        X, y = shuffle(X, y, random_state=epoch)\n",
        "        for i in range(0,X.shape[0],self.batch_size):\n",
        "          x_batch = X[i:i + self.batch_size]\n",
        "          y_batch = y[i:i + self.batch_size]\n",
        "          self.fit_batch(x_batch, y_batch,(i/self.batch_size))\n",
        "        y_true = self.process_y(y)\n",
        "        acc_t=(np.mean(self.predict(X).argmax(axis=-1)==y_true.argmax(axis=-1)))\n",
        "        #print(y_true)\n",
        "        print(\"Epoch: \"+str(epoch+1)+\", Train accuracy: \"+str(acc_t))\n",
        "\n",
        "    def predict(self,X):\n",
        "      y_pred = self.forward(X)\n",
        "      return y_pred\n",
        "    \n",
        "    def evaluate(self, y_pred, y_true):\n",
        "        y_true = self.process_y(y_true)\n",
        "        acc=(np.mean(y_pred.argmax(axis=-1)==y_true.argmax(axis=-1)))\n",
        "        loss = self.loss_function.loss(y_pred,y_true)\n",
        "        l2reg = 0\n",
        "        for layer in self.network:\n",
        "            l2reg += self.wd*np.sum(np.square(layer.w))\n",
        "        l2reg = l2reg/(2.0 * y_true.shape[0])\n",
        "        return acc, loss+l2reg\n",
        "    \n",
        "## WEIGHT INITIALIZER\n",
        "class randwb():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def init_w(self, prev_n, curr_n):\n",
        "    return np.random.randn(prev_n, curr_n)\n",
        "\n",
        "class xavier():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def init_w(self, prev_n, curr_n):\n",
        "    return np.random.normal(0,np.sqrt(6/(prev_n+curr_n)),(prev_n, curr_n))\n",
        "\n",
        "##  OPTIMIZERS\n",
        "class SGDOptim():\n",
        "    def __init__(self, eta=0.01, wd = 0):\n",
        "        self.eta = eta\n",
        "        self.wd = wd\n",
        "        self.name = 'sgd'\n",
        "\n",
        "    def update(self, w, b, dw, db, t=0):\n",
        "        ## dw, db are from current minibatch\n",
        "        ## update weights and biases\n",
        "        w = w - self.eta*(dw) - self.eta*(self.wd)*(w)\n",
        "        b = b - self.eta*(db) \n",
        "        return w, b\n",
        "\n",
        "class MomentGDOptim():\n",
        "    def __init__(self, eta=0.01, gamma=0.9, wd=0):\n",
        "        self.v_w, self.v_b = 0, 0\n",
        "        self.gamma = gamma\n",
        "        self.eta = eta\n",
        "        self.wd = wd\n",
        "        self.name = 'mgd'\n",
        "\n",
        "    def update(self, w, b, dw, db, t=0):\n",
        "        ## dw, db are from current minibatch\n",
        "        ## momentum \n",
        "        self.v_w = self.gamma*self.v_w + self.eta*dw\n",
        "        self.v_b = self.gamma*self.v_b + self.eta*db\n",
        "\n",
        "        ## update weights and biases\n",
        "        w = w - self.v_w - self.eta*(self.wd)*(w)\n",
        "        b = b - self.v_b \n",
        "        return w, b\n",
        "\n",
        "class NAGDOptim():\n",
        "    def __init__(self, eta=0.01, gamma=0.9, wd=0):\n",
        "        self.v_w, self.v_b = 0, 0\n",
        "        self.prev_vw, self.prev_vb = 0,0\n",
        "        self.gamma = gamma\n",
        "        self.eta = eta\n",
        "        self.wd = wd\n",
        "        self.name = 'nag'\n",
        "\n",
        "    def partial(self):\n",
        "        self.v_w = self.gamma*self.prev_vw\n",
        "        self.v_b = self.gamma*self.prev_vb\n",
        "        return self.v_w, self.v_b\n",
        "\n",
        "    def update(self, w, b, dw, db, t=0):\n",
        "        ## dw, db are from current minibatch\n",
        "        ## momentum \n",
        "        self.v_w = self.gamma*self.prev_vw + self.eta*dw\n",
        "        self.v_b = self.gamma*self.prev_vb + self.eta*db\n",
        "        \n",
        "        ## update weights and biases\n",
        "        w = w - self.eta*dw - self.eta*(self.wd)*(w)\n",
        "        b = b - self.eta*db\n",
        "\n",
        "        ##\n",
        "        self.prev_vw, self.prev_vb = self.v_w, self.v_b\n",
        "        return w, b\n",
        "\n",
        "class RMSOptim():\n",
        "    def __init__(self, eta=0.1, beta1=0.9, eps=1e-8, wd=0):\n",
        "        self.v_w, self.v_b = 0, 0\n",
        "        self.beta1 = beta1\n",
        "        self.eps = eps\n",
        "        self.eta = eta\n",
        "        self.wd = wd\n",
        "        self.name = 'rms'\n",
        "\n",
        "    def update(self, w, b, dw, db, t=0):\n",
        "        ## dw, db are from current minibatch\n",
        "        ## momentum beta 1\n",
        "        self.v_w = self.beta1*self.v_w + (1-self.beta1)*(dw**2)\n",
        "        self.v_b = self.beta1*self.v_b + (1-self.beta1)*(db**2)\n",
        "\n",
        "        ## update weights and biases\n",
        "        w = w - (self.eta/(np.sqrt(self.v_w+self.eps)))*dw - self.eta*(self.wd)*(w)\n",
        "        b = b - (self.eta/(np.sqrt(self.v_b+self.eps)))*db\n",
        "        return w, b\n",
        "\n",
        "class adamOptim():\n",
        "    def __init__(self, eta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, wd=0):\n",
        "        self.m_dw, self.v_dw = 0, 0\n",
        "        self.m_db, self.v_db = 0, 0\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.eta = eta\n",
        "        self.wd = wd\n",
        "        self.name = 'adam'\n",
        "    def update(self, t, w, b, dw, db):\n",
        "        ## dw, db are from current minibatch\n",
        "        ## momentum beta 1\n",
        "        self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw\n",
        "        self.m_db = self.beta1*self.m_db + (1-self.beta1)*db\n",
        "\n",
        "        ## rms beta 2\n",
        "        self.v_dw = self.beta2*self.v_dw + (1-self.beta2)*(dw**2)\n",
        "        self.v_db = self.beta2*self.v_db + (1-self.beta2)*(db**2)\n",
        "\n",
        "        ## bias correction\n",
        "        m_dw_hat = self.m_dw/(1-self.beta1**(t+1))\n",
        "        m_db_hat = self.m_db/(1-self.beta1**(t+1))\n",
        "        v_dw_hat = self.v_dw/(1-self.beta2**(t+1))\n",
        "        v_db_hat = self.v_db/(1-self.beta2**(t+1))\n",
        "\n",
        "        ## update weights and biases\n",
        "        w = w - (self.eta/(np.sqrt(v_dw_hat+self.epsilon)))*(m_dw_hat) - self.eta*(self.wd)*(w)\n",
        "        b = b - (self.eta/(np.sqrt(v_db_hat+self.epsilon)))*(m_db_hat)\n",
        "        return w, b\n",
        "\n",
        "class nadamOptim():\n",
        "    def __init__(self, eta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, wd = 0):\n",
        "        self.m_dw, self.v_dw = 0, 0\n",
        "        self.m_db, self.v_db = 0, 0\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.eta = eta\n",
        "        self.wd = wd\n",
        "        self.name='nadam'\n",
        "\n",
        "    def update(self, t, w, b, dw, db):\n",
        "        ## dw, db are from current minibatch\n",
        "        ## momentum beta 1\n",
        "        self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw\n",
        "        self.m_db = self.beta1*self.m_db + (1-self.beta1)*db\n",
        "\n",
        "        ## rms beta 2\n",
        "        self.v_dw = self.beta2*self.v_dw + (1-self.beta2)*(dw**2)\n",
        "        self.v_db = self.beta2*self.v_db + (1-self.beta2)*(db**2)\n",
        "\n",
        "        ## bias correction\n",
        "        m_dw_hat = self.m_dw/(1-self.beta1**(t+1))\n",
        "        m_db_hat = self.m_db/(1-self.beta1**(t+1))\n",
        "        v_dw_hat = self.v_dw/(1-self.beta2**(t+1))\n",
        "        v_db_hat = self.v_db/(1-self.beta2**(t+1))\n",
        "\n",
        "        ## nesterov\n",
        "        m_dw_m = self.beta1*m_dw_hat + ((1-self.beta1)*(dw))/(1-self.beta1**(t+1))\n",
        "        m_db_m = self.beta1*m_db_hat + ((1-self.beta1)*(db))/(1-self.beta1**(t+1))\n",
        "\n",
        "        ## update weights and biases\n",
        "        w = w - (self.eta/(np.sqrt(v_dw_hat+self.epsilon)))*(m_dw_m) - self.eta*(self.wd)*(w)\n",
        "        b = b - (self.eta/(np.sqrt(v_db_hat+self.epsilon)))*(m_db_m)\n",
        "        return w, b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get training and testing vectors \n",
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "\n",
        "trainX = trainX.reshape(60000, 784)/255.0\n",
        "testX = testX.reshape(10000, 784)/255.0\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(trainX, trainy, test_size=0.1, random_state=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLvh5PaI_Rc5"
      },
      "source": [
        "!pip install wandb --upgrade\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10lFa7lEwmPz"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'RANDOM', #grid, random, bayes\n",
        "    'metric': {\n",
        "      'name': 'val_loss',\n",
        "      'goal': 'minimize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [5, 10]\n",
        "        },\n",
        "        'num_layers': {\n",
        "            'values': [3, 4, 5]\n",
        "        }\n",
        "        'layer_size': {\n",
        "            'values': [16, 32, 64]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0, 0.0005, 0.5]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['nadam', 'adam', 'rms', 'nag', 'mgd', 'sgd']\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [64, 32, 16]\n",
        "        },\n",
        "        'wb_initializer':{\n",
        "            'values': ['random', 'xavier']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['sigmoid', 'relu', 'tanh']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"cs6910-a1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yT8iQAtuULb"
      },
      "source": [
        "def train(config=None):\n",
        "    with wandb.init(config = config):\n",
        "        config = wandb.config\n",
        "        neuronlist = []\n",
        "        for i in range(config.num_layers):\n",
        "            neuronlist.append([config.layer_size, config.activation])\n",
        "        parameters = dict(input_size = 784, output_size = 10, neuronlist = neuronlist,\n",
        "                  batch_size = config.batch_size, epochs = config.epochs, optimizer = config.optimizer,\n",
        "                  learning_rate = config.learning_rate, wb_initializer = config.wb_initializer, weight_decay = config.weight_decay,\n",
        "                  loss_function = 'crossentropy')\n",
        "        wandb.run.name = 'hn-'+str(config.num_layers)+'_hs-'+str(config.layer_size)+'_a-'+config.activation+'_bs-'+str(config.batch_size)+'_o-'+config.optimizer\n",
        "        fnn = nn(**parameters)\n",
        "        (trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "        trainX = trainX.reshape(60000, 784)/255.0\n",
        "        testX = testX.reshape(10000, 784)/255.0\n",
        "        X_train, X_val, y_train, y_val = train_test_split(trainX, trainy, test_size=0.1, random_state=0)\n",
        "        for epoch in range(config.epochs):\n",
        "            fnn.fit_epoch(X_train, y_train, epoch)\n",
        "            y_vpred = fnn.predict(X_val)\n",
        "            val_acc, val_loss = fnn.evaluate(y_vpred, y_val)\n",
        "            y_tpred = fnn.predict(testX)\n",
        "            acc, loss = fnn.evaluate(y_tpred, testy)\n",
        "            wandb.log({'val_loss': val_loss, 'val_accuracy': val_acc, \n",
        "                       'loss': loss, 'accuracy': acc, 'epoch': epoch+1})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJe9eT1J_MgQ"
      },
      "source": [
        "wandb.agent(sweep_id, train, count=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8IVbwjww5pI"
      },
      "source": [
        "# training model on 100% data and testing it\n",
        "neuronlist = []\n",
        "for i in range(4):\n",
        "    neuronlist.append([64, 'tanh'])\n",
        "\n",
        "parameters = dict(input_size = 784, output_size = 10, neuronlist = neuronlist,\n",
        "                  batch_size = 32, epochs = 10, optimizer = 'rms',\n",
        "                  learning_rate = 0.001, wb_initializer = 'xavier', weight_decay = 0.0005,\n",
        "                  loss_function = 'crossentropy')\n",
        "fnn = nn(**parameters)\n",
        "\n",
        "t1 = time.time()\n",
        "fnn.fit(x_train, Y_train)\n",
        "t2 = time.time()\n",
        "print(\"\\nTime taken to train: \"+str(t2-t1))\n",
        "\n",
        "y_t = fnn.predict(X_test)\n",
        "acc, loss = fnn.evaluate(y_t, y_test)\n",
        "print(\"Test accuracy: \"+str(acc)+\", test loss: \"+str(loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAIutaDVz8vL"
      },
      "source": [
        "# On MNIST data\n",
        "from keras.datasets import mnist\n",
        "# get training and testing vectors \n",
        "(trainX, Y_train), (testX, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = trainX.reshape(60000, 784)/255.0\n",
        "X_test = testX.reshape(10000, 784)/255.0\n",
        "\n",
        "\n",
        "fnn = nn(**parameters)\n",
        "fnn.fit(x_train, Y_train, x_train, Y_train)\n",
        "\n",
        "y_t = fnn.predict(X_test)\n",
        "acc, loss = fnn.evaluate(y_t, y_test)\n",
        "print(\"\\nTest accuracy: \"+str(acc)+\", test loss: \"+str(loss))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}