{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import RNN, SimpleRNN\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
    "!tar -xf /content/dakshina_dataset_v1.0.tar\n",
    "!apt-get install -y fonts-lohit-deva\n",
    "!fc-list :lang=hi family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "# Path to the data txt file on disk.\n",
    "train_path = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
    "val_path = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
    "test_path = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '\\t' + w + '\\n'\n",
    "  return w\n",
    "\n",
    "def create_dataset(path):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')]\n",
    "                for line in lines]\n",
    "\n",
    "  return zip(*word_pairs)\n",
    "\n",
    "def tokenize(lang, lang_tokenizer=None):\n",
    "  if lang_tokenizer is None:\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(path, inp_lang_tokenizer=None, targ_lang_tokenizer=None):\n",
    "  # create input, output pairs\n",
    "  targ_lang, inp_lang, _ = create_dataset(path)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang, inp_lang_tokenizer)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang, targ_lang_tokenizer)\n",
    "  target_str = tf.convert_to_tensor(targ_lang)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor, target_str))\n",
    "  dataset = dataset.shuffle(len(dataset))\n",
    "  \n",
    "  return dataset, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, inp_tokenizer, targ_tokenizer = load_dataset(train_path)\n",
    "val_dataset, _, _ = load_dataset(val_path, inp_tokenizer, targ_tokenizer)\n",
    "test_dataset, _, _ = load_dataset(test_path, inp_tokenizer, targ_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer(layer_name, num_cells, dropout, return_sequences, return_state):\n",
    "  if layer_name==\"RNN\":\n",
    "    return SimpleRNN(num_cells, return_sequences=return_sequences, return_state=return_state, dropout=dropout)\n",
    "  elif layer_name==\"LSTM\":\n",
    "    return LSTM(num_cells, return_sequences=return_sequences, return_state=return_state, dropout=dropout)\n",
    "  elif layer_name==\"GRU\":\n",
    "    return GRU(num_cells, return_sequences=return_sequences, return_state=return_state, dropout=dropout)\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, layer_name, evsize, embedd_size, nlayers, num_encoder, bsize, dropout=0.0):\n",
    "    super(Encoder,self).__init__()\n",
    "    self.bsize=bsize\n",
    "    self.nlayers=nlayers\n",
    "    self.layer_name = layer_name\n",
    "    self.num_encoder=num_encoder\n",
    "    self.vocab_size = evsize\n",
    "    self.embedding=Embedding(evsize,embedd_size)\n",
    "    #self.layer=get_layer(layer_name,num_encoder, dropout, True, True)\n",
    "    self.layer_list=[]\n",
    "    for i in range(self.nlayers):\n",
    "        self.layer_list.append(get_layer(layer_name,num_encoder, dropout, True, True))\n",
    "\n",
    "  def call(self,x,hidden=None):\n",
    "    x=self.embedding(x)\n",
    "    x=self.layer_list[0](x, initial_state=hidden)\n",
    "    for layer in self.layer_list[1:]:\n",
    "      x = layer(x)\n",
    "    output, state_h = x[0], x[1:]\n",
    "    return output,state_h\n",
    "\n",
    "  def initialize_hidden_state(self, bsize=-1):\n",
    "    if bsize == -1:\n",
    "        bsize = self.bsize\n",
    "    if self.layer_name==\"LSTM\":\n",
    "        return [tf.zeros((bsize, self.num_encoder))]*2\n",
    "    return [tf.zeros((bsize,self.num_encoder))]\n",
    "\n",
    "  def from_embedd(self, x, hidden=None):\n",
    "    x=self.layer_list[0](x, initial_state=hidden)\n",
    "    for layer in self.layer_list[1:]:\n",
    "      x = layer(x)\n",
    "    output, state_h = x[0], x[1:]\n",
    "    return output,state_h\n",
    "    "
   ]
  }
 ]
}