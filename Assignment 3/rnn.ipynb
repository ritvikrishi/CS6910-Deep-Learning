{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import RNN, SimpleRNN\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting dataset for running on google colab\n",
    "!nvidia-smi\n",
    "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
    "!tar -xf /content/dakshina_dataset_v1.0.tar\n",
    "!apt-get install -y fonts-lohit-deva\n",
    "!fc-list :lang=hi family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "# Path to the data txt file on disk.\n",
    "train_path = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
    "val_path = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
    "test_path = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset preprocessing\n",
    "def preprocess_sentence(w):\n",
    "  w = '\\t' + w + '\\n'\n",
    "  return w\n",
    "\n",
    "def create_dataset(path):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')]\n",
    "                for line in lines]\n",
    "\n",
    "  return zip(*word_pairs)\n",
    "\n",
    "def tokenize(lang, lang_tokenizer=None):\n",
    "  if lang_tokenizer is None:\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(path, inp_lang_tokenizer=None, targ_lang_tokenizer=None):\n",
    "  # create input, output pairs\n",
    "  targ_lang, inp_lang, _ = create_dataset(path)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang, inp_lang_tokenizer)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang, targ_lang_tokenizer)\n",
    "  target_str = tf.convert_to_tensor(targ_lang)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor, target_str))\n",
    "  dataset = dataset.shuffle(len(dataset))\n",
    "  \n",
    "  return dataset, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset\n",
    "train_dataset, inp_tokenizer, targ_tokenizer = load_dataset(train_path)\n",
    "val_dataset, _, _ = load_dataset(val_path, inp_tokenizer, targ_tokenizer)\n",
    "test_dataset, _, _ = load_dataset(test_path, inp_tokenizer, targ_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer(layer_name, num_cells, dropout, return_sequences, return_state):\n",
    "  if layer_name==\"RNN\":\n",
    "    return SimpleRNN(num_cells, return_sequences=return_sequences, return_state=return_state, dropout=dropout)\n",
    "  elif layer_name==\"LSTM\":\n",
    "    return LSTM(num_cells, return_sequences=return_sequences, return_state=return_state, dropout=dropout)\n",
    "  elif layer_name==\"GRU\":\n",
    "    return GRU(num_cells, return_sequences=return_sequences, return_state=return_state, dropout=dropout)\n",
    "\n",
    "# encoder of our model\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, layer_name, evsize, embedd_size, nlayers, num_encoder, bsize, dropout=0.0):\n",
    "    super(Encoder,self).__init__()\n",
    "    self.bsize=bsize\n",
    "    self.nlayers=nlayers\n",
    "    self.layer_name = layer_name\n",
    "    self.num_encoder=num_encoder\n",
    "    self.vocab_size = evsize\n",
    "    self.embedding=Embedding(evsize,embedd_size)\n",
    "    #self.layer=get_layer(layer_name,num_encoder, dropout, True, True)\n",
    "    self.layer_list=[]\n",
    "    for i in range(self.nlayers):\n",
    "        self.layer_list.append(get_layer(layer_name,num_encoder, dropout, True, True))\n",
    "\n",
    "  def call(self,x,hidden=None):\n",
    "    x=self.embedding(x)\n",
    "    x=self.layer_list[0](x, initial_state=hidden)\n",
    "    for layer in self.layer_list[1:]:\n",
    "      x = layer(x)\n",
    "    output, state_h = x[0], x[1:]\n",
    "    return output,state_h\n",
    "\n",
    "  def initialize_hidden_state(self, bsize=-1):\n",
    "    if bsize == -1:\n",
    "        bsize = self.bsize\n",
    "    if self.layer_name==\"LSTM\":\n",
    "        return [tf.zeros((bsize, self.num_encoder))]*2\n",
    "    return [tf.zeros((bsize,self.num_encoder))]\n",
    "\n",
    "  def from_embedd(self, x, hidden=None):\n",
    "    x=self.layer_list[0](x, initial_state=hidden)\n",
    "    for layer in self.layer_list[1:]:\n",
    "      x = layer(x)\n",
    "    output, state_h = x[0], x[1:]\n",
    "    return output,state_h\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder of our model\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, layer_name, dvsize, embedd_size, nlayers, num_units, bsize, dropout=0.0, attention=False):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = bsize\n",
    "    self.layer_name = layer_name\n",
    "    self.vocab_size = dvsize\n",
    "    self.dec_units = num_units\n",
    "    self.dropout=dropout\n",
    "    self.nlayers = nlayers\n",
    "    self.attention=attention\n",
    "    self.embedding = tf.keras.layers.Embedding(input_dim=dvsize,output_dim= embedd_size)\n",
    "    #self.layer = get_layer(layer_name, num_units, dropout, True, True)\n",
    "    self.layer_list = []\n",
    "    for i in range(nlayers-1):\n",
    "      self.layer_list.append(get_layer(layer_name, num_units, dropout, True, True))\n",
    "    self.layer_list.append(get_layer(layer_name, num_units, dropout, False, True))\n",
    "    self.dense = Dense(self.vocab_size, activation=\"softmax\")\n",
    "    self.flatten=Flatten()\n",
    "    if self.attention:\n",
    "        self.attention_layer = BahdanauAttention(num_units)\n",
    "\n",
    "  def call(self,x,hidden,enc_output=None):\n",
    "    x=self.embedding(x)\n",
    "\n",
    "    if self.attention:\n",
    "      context, att_wts = self.attention_layer(hidden, enc_output)\n",
    "      x = tf.concat([tf.expand_dims(context, 1), x], axis=-1)\n",
    "    else:\n",
    "      att_wts=None\n",
    "    \n",
    "    x = self.layer_list[0](x, initial_state = hidden)\n",
    "    for layer in self.layer_list[1:]:\n",
    "        x = layer(x)\n",
    "    output = x[0]\n",
    "    dec_h = x[1:]\n",
    "    output = self.dense(self.flatten(output))\n",
    "\n",
    "    return output, dec_h, att_wts\n",
    "\n",
    "  def rnn_output(self, x, hidden, enc_output=None):\n",
    "    x=self.embedding(x)\n",
    "\n",
    "    if self.attention:\n",
    "      context, att_wts = self.attention_layer(hidden, enc_output)\n",
    "      x = tf.concat([tf.expand_dims(context, 1), x], axis=-1)\n",
    "    else:\n",
    "      att_wts=None\n",
    "    \n",
    "    x = self.layer_list[0](x, initial_state = hidden)\n",
    "    for layer in self.layer_list[1:]:\n",
    "        x = layer(x)\n",
    "    output = x[0]\n",
    "    dec_h = x[1:]\n",
    "    return output, dec_h, att_wts\n",
    "\n",
    "# encoder-decoder model class\n",
    "class make_model():\n",
    "  def __init__(self, embedded_dim, enc_layers, dec_layers, layer_name, num_units, dropout, attention=False, wandbcallback=False):\n",
    "    evsize = len(inp_tokenizer.word_index) + 1\n",
    "    dvsize = len(targ_tokenizer.word_index) + 1\n",
    "    self.callback = wandbcallback\n",
    "    self.batch_size = 64\n",
    "    self.inp_tokenizer = inp_tokenizer\n",
    "    self.targ_tokenizer = targ_tokenizer\n",
    "    self.encoder = Encoder(layer_name, evsize, embedded_dim, enc_layers, num_units, self.batch_size, dropout)\n",
    "    self.decoder = Decoder(layer_name, dvsize, embedded_dim, dec_layers, num_units, self.batch_size, dropout, attention)\n",
    "  \n",
    "  def get_vsize(self):\n",
    "    evsize = len(inp_tokenizer.word_index) + 1\n",
    "    dvsize = len(targ_tokenizer.word_index) + 1\n",
    "    return evsize, dvsize\n",
    "  \n",
    "  # DO NOT USE\n",
    "  def create(self):\n",
    "    enc_inputs = Input(shape=(None,))\n",
    "    #enc_state = self.encoder.initialize_hidden_state()\n",
    "    enc_output, state_h = self.encoder(enc_inputs)\n",
    "    enc_states = [state_h]\n",
    "\n",
    "    dec_inputs = Input(shape=(None,))\n",
    "    #dec_input = tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]]*self.batch_size ,1)\n",
    "    dec_state = state_h\n",
    "    dec_output, att_wts = self.decoder(dec_inputs, dec_state, enc_output)\n",
    "\n",
    "    self.model = Model([enc_inputs, dec_inputs], dec_output)\n",
    "    self.model.summary()    \n",
    "  \n",
    "  def build(self, loss, optimizer, metric):\n",
    "    self.loss_function = loss\n",
    "    self.optimizer = optimizer\n",
    "    self.metric = metric\n",
    "\n",
    "  def decoder_input(self, ii=None):\n",
    "    if ii is None:\n",
    "        return tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]]*self.batch_size,1)\n",
    "    return tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]]*ii,1)\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(self,input,target,encoder_hidden):\n",
    "    loss=0\n",
    "    with tf.GradientTape() as tape:\n",
    "      encoder_output,encoder_hidden=self.encoder(input,encoder_hidden)\n",
    "      decoder_hidden=encoder_hidden\n",
    "      decoder_input=tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]]*self.batch_size ,1)\n",
    "      for t in range(1,target.shape[1]):\n",
    "        predictions,decoder_hidden,attnweights=self.decoder(decoder_input,decoder_hidden,encoder_output)\n",
    "        loss+=self.loss_function(target[:,t],predictions)\n",
    "        self.metric.update_state(target[:,t], predictions)\n",
    "        predictions=tf.argmax(predictions,1)#get max index\n",
    "        decoder_input=tf.expand_dims(predictions,1)#to match shape of decoder input\n",
    "      batch_loss=(loss/int(target.shape[1]))\n",
    "      variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "      gradients = tape.gradient(loss, variables)\n",
    "      self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss, self.metric.result()\n",
    "  \n",
    "  def fit(self, dataset, val_dataset, epochs, iswandb=False, teacher_ratio = 1.0):\n",
    "    self.epochs = epochs\n",
    "    steps_per_epoch = len(dataset) // self.batch_size\n",
    "    steps_per_epoch_val = len(val_dataset) // self.batch_size\n",
    "    dataset = dataset.batch(self.batch_size, drop_remainder=True)\n",
    "    val_dataset = val_dataset.batch(self.batch_size, drop_remainder=True)\n",
    "    sample_inp, sample_targ , _= next(iter(dataset))\n",
    "    self.max_inp_len = sample_inp.shape[1]\n",
    "    self.max_targ_len = sample_targ.shape[1]\n",
    "    self.teacher_ratio = teacher_ratio\n",
    "\n",
    "    for epoch in range(self.epochs):\n",
    "      start=time.time()\n",
    "      encoder_hidden=self.encoder.initialize_hidden_state()\n",
    "      total_loss=0\n",
    "      total_acc = 0\n",
    "      self.metric.reset_states()\n",
    "      for (batch,(input,target,_dh)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss, batch_acc =self.train_step(input,target,encoder_hidden)\n",
    "        total_loss += batch_loss\n",
    "        total_acc += batch_acc\n",
    "      loss = total_loss / steps_per_epoch\n",
    "      acc = total_acc / steps_per_epoch\n",
    "\n",
    "      encoder_hidden=self.encoder.initialize_hidden_state()\n",
    "      tot_valloss=0\n",
    "      tot_valacc = 0\n",
    "      self.metric.reset_states()\n",
    "      for (batch,(input,target,_)) in enumerate(val_dataset.take(steps_per_epoch_val)):\n",
    "        batch_loss, batch_acc =self.test_step(input,target,encoder_hidden)\n",
    "        tot_valloss += batch_loss\n",
    "        tot_valacc += batch_acc\n",
    "      val_loss = tot_valloss / steps_per_epoch_val\n",
    "      val_acc = tot_valacc / steps_per_epoch_val\n",
    "      print(\"Epoch: \"+str(epoch+1)+\"/\"+str(self.epochs)+\" trained,\\t Time taken : \"+str(int(time.time()-start))+\"s\")\n",
    "      print(\"loss : {0:.4f}  acc : {1:.4f},   val_loss : {2:.4f}  val_acc : {3:.4f}\".format(loss, acc, val_loss, val_acc))\n",
    "      if self.callback == True:\n",
    "        wandb.log({'val_loss': val_loss, 'val_accuracy': val_acc, \n",
    "                    'loss': loss, 'accuracy': acc, 'epoch': epoch+1})\n",
    "    print(\"Training done.\")\n",
    "\n",
    "\n",
    "  @tf.function\n",
    "  def test_step(self, input, target, encoder_hidden):\n",
    "    loss=0\n",
    "    encoder_output,encoder_hidden=self.encoder(input,encoder_hidden)\n",
    "    decoder_hidden=encoder_hidden\n",
    "    decoder_input=self.decoder_input()\n",
    "    for t in range(1,target.shape[1]):\n",
    "      predictions,decoder_hidden,attnweights=self.decoder(decoder_input,decoder_hidden,encoder_output)\n",
    "      loss+=self.loss_function(target[:,t],predictions)\n",
    "      self.metric.update_state(target[:,t], predictions)\n",
    "      predictions=tf.argmax(predictions,1)#get max index\n",
    "      decoder_input=tf.expand_dims(predictions,1)#to match shape of decoder input\n",
    "    batch_loss=(loss/int(target.shape[1]))\n",
    "    return batch_loss, self.metric.result()\n",
    "\n",
    "  # get prediction score on dataset (char-level)\n",
    "  def predict(self, test_dataset):\n",
    "      steps_per_epoch = len(test_dataset) // self.batch_size\n",
    "      dataset = test_dataset.batch(self.batch_size, drop_remainder=True)\n",
    "      encoder_hidden=self.encoder.initialize_hidden_state()\n",
    "      total_loss=0\n",
    "      total_acc=0\n",
    "      self.metric.reset_states()\n",
    "      for (batch,(input,target,_)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss, batch_acc =self.test_step(input,target,encoder_hidden)\n",
    "        total_loss += batch_loss\n",
    "        total_acc += batch_acc\n",
    "      avg_loss = total_loss / steps_per_epoch\n",
    "      avg_acc = total_acc / steps_per_epoch\n",
    "      print(\"Character level -  loss : {0:.4f}   acc : {1:.4f}\".format(avg_loss, avg_acc))\n",
    "\n",
    "  def get_input(self, word):\n",
    "    word = '\\t'+word+'\\n'\n",
    "    input = inp_tokenizer.texts_to_sequences([word])\n",
    "    input = tf.keras.preprocessing.sequence.pad_sequences(input,maxlen=self.max_inp_len,padding=\"post\")\n",
    "    #print(input.dtype)\n",
    "    return input\n",
    "\n",
    "  def get_next_char(self, predictions):\n",
    "    # print(predictions.shape)\n",
    "    x=predictions.numpy().item()\n",
    "    flag=0\n",
    "    if x==0:\n",
    "      x=5\n",
    "      flag=1\n",
    "    return self.targ_tokenizer.index_word[x],flag\n",
    "\n",
    "  # transliterate one word\n",
    "  def transliterate(self,word,val=False):\n",
    "    input = self.get_input(word)\n",
    "    new_word=''\n",
    "    attnweightplot=np.zeros((self.max_targ_len,self.max_inp_len))\n",
    "    encoder_hidden=self.encoder.initialize_hidden_state(1)\n",
    "    encoder_output,encoder_hidden=self.encoder(input,encoder_hidden)\n",
    "    decoder_hidden=encoder_hidden\n",
    "    decoder_input=self.decoder_input(1)\n",
    "    boolflag=0\n",
    "    for t in range(1,self.max_targ_len):\n",
    "      predictions,decoder_hidden,attnweights=self.decoder(decoder_input,decoder_hidden,encoder_output)\n",
    "      predictions=tf.argmax(predictions,1)#get max index\n",
    "      if val==True:\n",
    "        attnweights=tf.reshape(attnweights,(-1,))\n",
    "        attnweightplot[t]=attnweights.numpy()\n",
    "      next,flag=self.get_next_char(predictions)\n",
    "      boolflag+=flag\n",
    "      new_word+=next\n",
    "      # next=self.targ_tokenizer.index_word[predictions.numpy().item()]\n",
    "      #print(next)\n",
    "      if next == \"\\n\":\n",
    "        new_word = new_word[:-1]\n",
    "        break\n",
    "      decoder_input=tf.expand_dims(predictions,1)#to match shape of decoder input\n",
    "    return new_word,boolflag,attnweightplot\n",
    "\n",
    "  def plot_heatmap(self,word, wandbcallback=False):\n",
    "    new_word,boolflag,attnweightplot=self.transliterate(word,True)\n",
    "    xx=list(word)\n",
    "    yy=list(new_word)\n",
    "    attention=attnweightplot[:len(yy),:len(xx)]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    hindi_font=FontProperties(fname=\"/usr/share/fonts/truetype/lohit-devanagari/Lohit-Devanagari.ttf\")\n",
    "    ax.set_xticklabels([''] + yy, fontproperties=hindi_font, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + xx,fontdict=fontdict)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    if wandbcallback == True:\n",
    "      wandb.log({\"att_hm\": plt})\n",
    "    else:\n",
    "      plt.show()\n",
    "\n",
    "  def word_level(self,name):\n",
    "    test_tsv = pd.read_csv(test_path, sep=\"\\t\", header=None)\n",
    "    inputs = test_tsv[1].astype(str).tolist()\n",
    "    targets = test_tsv[0].astype(str).tolist()\n",
    "\n",
    "    outputs=[]\n",
    "    correctness=[]\n",
    "    for word in inputs:\n",
    "      z=model.transliterate(word)\n",
    "      x=z[0]\n",
    "      y=z[1]\n",
    "      outputs.append(x)\n",
    "      correctness.append(y)\n",
    "    acc1=np.sum(np.asarray(outputs) == np.array(targets)) / len(outputs)\n",
    "    cor=[]\n",
    "    for i in range(len(outputs)):\n",
    "      if outputs[i] == targets[i]:\n",
    "        cor.append(outputs[i])\n",
    "    \n",
    "    correct_outputs=[]\n",
    "    correct_targets=[]\n",
    "    for i in range(len(outputs)):\n",
    "      if correctness[i] == 0:\n",
    "        correct_outputs.append(outputs[i])\n",
    "        correct_targets.append(targets[i])\n",
    "    acc2=np.sum(np.asarray(correct_outputs) == np.array(correct_targets)) / len(correct_outputs)\n",
    "    print(\"Word level acc : \" + str(acc1))\n",
    "    print(\"acc2 : \" +str(acc2))\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"inputs\"] = inputs\n",
    "    df[\"targets\"] = targets\n",
    "    df[\"outputs\"] = outputs\n",
    "    df.to_csv(name+'.csv')\n",
    "    df2=pd.DataFrame()\n",
    "    df2[\"correct-preds\"]=cor\n",
    "    df2.to_csv(name+'-correct.csv')\n",
    "   \n",
    "  def get_connectivity(self, word):\n",
    "    input = self.get_input(word)\n",
    "    new_word=''\n",
    "    gradlist = []\n",
    "    enc_hidden = self.encoder.initialize_hidden_state(1)\n",
    "    embedd_in=self.encoder.embedding(input)\n",
    "    with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
    "      tape.watch(embedd_in)\n",
    "      enc_out, enc_hidden = self.encoder.from_embedd(embedd_in, enc_hidden)\n",
    "      dec_hidden = enc_hidden\n",
    "      dec_input = self.decoder_input(1)\n",
    "      for t in range(1, self.max_targ_len):\n",
    "        rnn_out, dec_hidden, _ = self.decoder.rnn_output(dec_input, dec_hidden, enc_out)\n",
    "        predictions = self.decoder.dense(self.decoder.flatten(rnn_out))\n",
    "        predictions = tf.argmax(predictions, 1)\n",
    "        next = self.get_next_char(predictions)[0]\n",
    "        #print(next)\n",
    "        if next == \"\\n\":\n",
    "          break\n",
    "        new_word += next\n",
    "        gradlist.append(tape.gradient(rnn_out, embedd_in)[0])\n",
    "        dec_input = tf.expand_dims(predictions, 1)\n",
    "      return new_word, gradlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for running on custom model\n",
    "model = make_model(128, 2, 2, 'GRU', 128, 0.3, True)\n",
    "model.build(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                optimizer = tf.keras.optimizers.Adam(),\n",
    "                metric = tf.keras.metrics.SparseCategoricalAccuracy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, val_dataset,7)\n",
    "model.predict(test_dataset)\n",
    "\n",
    "# uncomment to find prediction at word level\n",
    "#model.word_level('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting attention heatmaps\n",
    "#ll=['ankit','ank']\n",
    "for x in ll:\n",
    "  model.plot_heatmap(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb sweeps in colab\n",
    "!pip install wandb -q\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep config for logging on wandb\n",
    "sweep_config = {\n",
    "    'method': 'random', #grid, bayes\n",
    "    'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'embedded_dim' : {\n",
    "            'values' : [16,32,64,256]\n",
    "        },\n",
    "        'num_units' : {\n",
    "            'values' : [16,32,64,256]\n",
    "        },\n",
    "        'enc_layers' : {\n",
    "            'values' : [1,2,3]\n",
    "        },\n",
    "        'dec_layers' : {\n",
    "            'values' : [1,2,3]\n",
    "        },\n",
    "        'dropout' : {\n",
    "            'values' : [0.0,0.2,0.3]\n",
    "        },\n",
    "        'layer_name' : {\n",
    "            'values' : ['RNN', 'GRU', 'LSTM']\n",
    "        },\n",
    "        'attention':{\n",
    "            'values': [False]\n",
    "        },\n",
    "        'epochs':{\n",
    "            'values': [10,15]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config,project = \"cs6910-a3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb train function\n",
    "def train():\n",
    "    # # Default values for hyper-parameters we're going to sweep over\n",
    "    some_parameters = {\n",
    "      'wandbcallback' : True\n",
    "    }\n",
    "    # Initialize a new wandb run\n",
    "    wandb.init(project = \"cs6910-a3\", config=some_parameters)\n",
    "    config = wandb.config\n",
    "    runname = \"ln-\"+str(config.layer_name)+\"_nu-\"+str(config.num_units)+\"_ed-\"+str(config.embedded_dim)+\"_el-\"+str(config.enc_layers)\n",
    "    runname += \"_dl-\"+str(config.dec_layers) + \"_dr-\"+str(config.dropout)\n",
    "    wandb.run.name = runname\n",
    "    model = make_model(config.embedded_dim, config.enc_layers, config.dec_layers, config.layer_name, config.num_units,\n",
    "                       config.dropout, config.attention, True)\n",
    "    model.build(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                optimizer = tf.keras.optimizers.Adam(),\n",
    "                metric = tf.keras.metrics.SparseCategoricalAccuracy())\n",
    "    model.fit(train_dataset, val_dataset, epochs = config.epochs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function = train, count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising connectivity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "import matplotlib\n",
    "from IPython.display import HTML as html_print\n",
    "from IPython.display import display\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# get html element\n",
    "def cstr(s, color='black'):\n",
    "    if s == ' ':\n",
    "      return \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(color, s)\n",
    "    else:\n",
    "      return \"<text style=color:#000;background-color:{}>{} </text>\".format(color, s)\n",
    "\t\n",
    "# print html\n",
    "def print_color(t):\n",
    "\t  display(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))\n",
    "\n",
    "# get appropriate color for value\n",
    "def get_clr(value):\n",
    "    colors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8'\n",
    "      '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
    "      '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
    "      '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
    "    value = int(value * 19)\n",
    "    if value == 19:\n",
    "        value -= 1\n",
    "    return colors[value]\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    z = 1/(1 + np.exp(-x)) \n",
    "    return z\n",
    "\n",
    "def visualize(grads, word, new_word):\n",
    "    print(\"Romanized:\", word)\n",
    "    print(\"Devanagari:\", new_word)\n",
    "    for i in range(len(new_word)):\n",
    "        print(\"Connectivity visualization for\", new_word[i],\":\")\n",
    "        text_colours = []\n",
    "        for j in range(len(grads[i])):\n",
    "            text = (word[j], get_clr(grads[i][j]))\n",
    "            text_colours.append(text)\n",
    "        print_color(text_colours)\n",
    "\n",
    "def get_activation(grads, word):\n",
    "    act_grad = []\n",
    "    for tensor in grads:\n",
    "        grad = tf.norm(tensor, axis=1)\n",
    "        grad = grad[:len(word)]\n",
    "        scaler = MinMaxScaler()\n",
    "        grad = tf.reshape(grad, (-1,1))\n",
    "        grad = scaler.fit_transform(grad)\n",
    "        act_grad.append(grad)\n",
    "    return act_grad\n",
    "\n",
    "\n",
    "def connectivity_vis(model, word):\n",
    "    new_word, grads = model.get_connectivity(word)\n",
    "    grad_act = get_activation(grads, word)\n",
    "    visualize(grad_act, word, new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_vis(model, 'bhagwan')"
   ]
  }
 ]
}